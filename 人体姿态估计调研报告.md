<center><font size = 6>人体姿态估计调研报告</font></center>

## 一、简述

​		人体姿态估计作为最近几年越来越火的研究目标，已经收到了广大研究人员的极大关注。同时，对人体姿态估计的研究对其他任务方面有很好的借鉴作用。基于此，本文调研了基于深度学习的人体姿态估计算法，并整理成此文档，借以日后浏览。

​		人体姿态是对人体的位置，位姿估计，简单一点就是估计人的手，脚，头部等关键点。在[深度学习论文榜单](https://www.paperswithcode.com/area/computer-vision/pose-estimation),把该任务分为14项任务，分别是

- 姿态估计，关键点检测，多人姿态估计

- 3D人体姿态估计，3D姿态估计

- 6D RGB姿态估计，6D姿态估计，6D RGBD 姿态估计

- 手势估计，头部姿态估计

- 人体姿态预测，动物姿态估计

- 基于RF的姿态估计

- 手部关节重建

  <img src=".\姿态估计img\image-20200723163435963.png" alt="image-20200723163435963" style="zoom:80%;" />

  <center>1-1 姿态估计任务</center>

  

  ​		关键点检测其实是一个普遍的任务，在很对地方都有应用，比如在车位检测中，每个角点可以看做一个关键点信息，在人脸关键点检测中，也是根据不同的位置设置不同的关键点。在本文中，关键点的检测对于人体姿态估计具有重要的作用，因此，对于关键点的研究是人体姿态估计的重点。

  <center class="half">
          <img src=".\姿态估计img\326_l2_1400.jpg" alt="326_l2_1400" width="250" style="margin-top:50px"/>                                   <img src=".\姿态估计img\u=2766369367,1424178344&amp;fm=26&amp;gp=0.jpg" alt="img" width="300" style="margin-left:45px"/>   
  </center>

  ​                                                       

  <center> 1-2 关键点例子</center>

  ​		人体姿态估计是对人体关键点求解的一个问题。首先，通过深度学习算法预测关键点信息，然后通过算法或者先验信息获取关键点之间的关联，通常的，姿态估计又分为几个重要的任务，分别是单人姿态估计，多人姿态估计，人体姿态跟踪，3D姿态估计。限于篇幅，本文会着重介绍单人姿态估计和多人姿态估计。

  ![image-20200724141753812](.\姿态估计img\image-20200724141753812.png)

  ​		姿态估计任务相对简单，如上图所示，在数据预处理过程中，输入原图，生成label，label是高斯热力图，当然这是现阶段大部分研究者的做法，至于为什么不直接回归点，在[知乎](https://zhuanlan.zhihu.com/p/69042249)中有人这样这样解释的：关键点预测主要有两种思路，Coordinate和Heatmap，Coordinate即直接将关键点坐标作为最后网络需要回归的目标，这种情况下可以直接得到每个坐标点的直接位置信息；Heatmap即将每一类坐标用一个概率图来表示，对图片中的每个像素位置都给一个概率，表示该点属于对应类别关键点的概率，比较自然的是，距离关键点位置越近的像素点的概率越接近1，距离关键点越远的像素点的概率越接近0，具体可以通过相应函数进行模拟，如Gaussian等，如果同一个像素位置距离不同关键点的距离大小不同，即  相对于不同关键点该位置的概率不一样，这时可以取Max或Average。对于两种Ground Truth的差别，Coordinate网络在本质上来说，需要回归的是每个关键点的一个相对于图片的offset，而长距离offset在实际学习过程中是很难回归的，误差较大，同时在训练中的过程，提供的监督信息较少，整个网络的收敛速度较慢；Heatmap网络直接回归出每一类关键点的概率，在一定程度上每一个点都提供了监督信息，网络能够较快的收敛，同时对每一个像素位置进行预测能够提高关键点的定位精度，在可视化方面，Heatmap也要优于Coordinate，除此之外，实践证明，Heatmap确实要远优于Coordinate。

## 二、关键点检测

### 1.简介

​		基于关键点的检测方法有很多，最常见的是关键点用在目标检测上，基于anchor-free的算法最近几年变得非常火热，其主要思想是抛弃原有的yolo系列的anchor的方式，在本文看来，目标检测从开始的rcnn一路走来，已经发展的非常成熟，rcnn的方式先提取目标框，然后做进一步的二阶段检测的方案已经不适应当前端到端检测的简单方式，而yolo和ssd系列的提出又是为了使用anchor指导检测框的形成，anchor的大小，多少都需要人为的确定，况且其中心思想也是中心点来负责检测框，既然如此，去掉anchor的anchor free方式岂不美哉。当然，本文看来，深度学习本身就是一个很简单的东西，是一个神经网络自主学习的过程，如果把他过于复杂化，不符合深度学习的初衷，与其设计不同的anchor，不如给网络增加中间监督信息，作者愚见。

这里举例当前的anchor-free方法：  CornerNet，ExtremeNet，FCOS。

目前的榜单介绍：

<img src=".\姿态估计img\image-20200723215349361.png" alt="image-20200723215349361" style="zoom: 80%;" />

### 2. 论文介绍

#### 2.1 CenterNet

**title：** CenterNet :Objects as Points

**code：**https://www.paperswithcode.com/paper/objects-as-points

**ref:** https://blog.csdn.net/c20081052/article/details/89358658

**time:**  **[v1]** Tue, 16 Apr 2019 17:54:26 UTC (8,592 KB)

​		在该文中，作者提出了一个高效的检测方案。用一个位于边框中心的点来表示对象还有其他属性，如对象大小、尺寸、3D范围、方向和姿态，然后直接从中心位置的图像特征进行回归。思路很简单，对于每一个图像输入，该网路目标是输出热力图![\hat{Y}\epsilon [0,1]^{\frac{W}{R}\times \frac{H}{R}\times C}](.\姿态估计img\times C}) ,其中R是输出步幅，C是关键点类型的数量，y表示是否是关键点。对于人体关键点，C=17，对与COCO检测，C=80。然后作者用了ResNet，hourglass，DLA等方式来预测结果。具体做法是用高斯核来构建真值，对于 Ground Truth（即GT）的关键点 c ,其位置为 ![p \epsilon R^{2}](.\姿态估计img\epsilon R^{2}) ，计算得到低分辨率（经过下采样）上对应的关键点 ![\tilde{p}=\left \lfloor \frac{p}{R} \right \rfloor](.\姿态估计img\rfloor) . 

![img](.\姿态估计img\20190417191729651.png)

如果两个重叠了，则选择元素级大的，损失函数使用focal loss。

![img](.\姿态估计img\20190417192157171.png)



当然，这样的方式会出现一个问题，就是中心点的位置是经过下采样的，与原始点会有偏差，因此作者还在每个中心点预测了一个局部偏移量 L1 loss。

![img](.\姿态估计img\20190417192941771.png)

其中，解释一下的意思就是在关键点的通道上额外增加了两个通道用来预测关键点的偏移量。

![img](.\姿态估计img\2019041719280132.png)

然后作者提出了多种检测方案：

![image-20200723203232965](.\姿态估计img\image-20200723203232965.png)

<center>2-1 CenterNet </center>

针对检测问题，输出C+4个通道，分别C是类别，4表示2个偏移量offset，2个表示w,h。推理的方法是将热力图上的所有响应点与其连接的8个临近点进行比较，如果该点响应值大于或等于其八个临近点值则保留，最后我们保留所有满足之前要求的前100个峰值点。

针对人体姿态检测。 人的姿态估计旨在估计 图像中每个人的k 个2D人的关节点位置（在COCO中，k是17，即每个人有17个关节点）。因此，中心点是 kx2维的，然后将每个关键点（关节点对应的点）参数化为相对于中心点的偏移。用到了L1 loss；通过给loss添加mask方式来无视那些不可见的关键点（关节点）。此处参照了slow-RCNN。这种方式的缺点是偏移量不够准确，因此作者额外预测了k+2(2表示与真值关节点的偏移)个热力图，表示每个关节点，然后每个关节点和上述的偏移量得到的点做匹配。如图2-1最下面的三张图，第一幅表示中心点自上而下的预测，第二幅表示关节点，关节点与第一幅图的关节点匹配，就能获取第二幅图每个关节点属于哪一个人。l2距离。所以最终的channel为k*2 + k+2

网络结构如图所示：

![image-20200723210618965](.\姿态估计img\image-20200723210618965.png)

**result:**

![image-20200723210021110](.\姿态估计img\image-20200723210021110.png)

![image-20200723210110062](.\姿态估计img\image-20200723210110062.png)

![image-20200723210349050](.\姿态估计img\image-20200723210349050.png)

#### 2.2 MSPN

**title：** Rethinking on Multi-Stage Networks for Human Pose Estimation

**time:**  arXiv:1901.00148v4 [cs.CV] 30 May 2019

​		该文提出了一种单人姿态关键点检测网络，分为多个阶段，是粗到细定位的过程，简单来说，每个阶段的高斯核大小不一样。两个改进，一个是设计了新型网络来预测关键点，第二个就是多阶段的定位。

![image-20200724152859057](.\姿态估计img\image-20200724152859057.png)

**result:**

![image-20200724153748157](.\姿态估计img\image-20200724153748157.png)

#### 2.3 HRNet

**title：** Rethinking on Multi-Stage Networks for Human Pose Estimation

**time:**  CVPR2019



<img src=".\姿态估计img\image-20200724154415327.png" alt="image-20200724154415327" style="zoom:67%;" />

​		一图说明一切。作者比较了hourglass，cascaded，simplebaseline 网络，最终设计了自己的网络结构。

<img src=".\姿态估计img\image-20200724154512233.png" alt="image-20200724154512233" style="zoom:67%;" />

#### 2.4 [Simple Base](https://www.paperswithcode.com/paper/simple-baselines-for-human-pose-estimation)

**title：** Simple Baselines for Human Pose Estimation and Tracking

**time:**  ECCV2018

​		该文提出了一种简单形式的关键点检测模型。这让其他文章有种一顿操作猛如虎，一看map没啥用的感觉，当然该文章在这一方面还有待思考，模型处在不一样大小也会影响性能。

![image-20200724163141740](.\姿态估计img\image-20200724163141740.png)

![image-20200724163502505](.\姿态估计img\image-20200724163502505.png)

#### 2.5 CCM

![image-20200724204807121](.\姿态估计img\image-20200724204807121.png)

## 三、单人姿态估计

​		单人姿态估计任务相对简单，输入是一个人的图片，然后通过算法检测该人的关键点，常见的数据集有[MPII](https://link.zhihu.com/?target=http%3A//human-pose.mpi-inf.mpg.de/), [LSP](https://link.zhihu.com/?target=https%3A//sam.johnson.io/research/lsp.html), [FLIC](https://link.zhihu.com/?target=https%3A//bensapp.github.io/flic-dataset.html), [LIP](https://link.zhihu.com/?target=http%3A//sysu-hcp.net/lip/)。其中MPII是2014年引进的，目前可以认为是单人姿态估计中最常用的benchmark， 使用的是PCKh的指标（可以认为预测的关键点与GT标注的关键点经过head size normalize后的距离）。但是经过这几年的算法提升，整体结果目前已经非常高了（最高的已经有93.9%了）。下面是单人姿态估计的结果图（图片来源于CPM的paper)：

![](.\姿态估计img\v2-f1bcb685269912c3d63592d855693c12_720w.jpg)



MPII结果：

![image-20200724193210422](C:\Users\duanlunbiao\AppData\Roaming\Typora\typora-user-images\image-20200724193210422.png)

#### 2.1 Soft-gated

**title：**[Toward fast and accurate human pose estimation via soft-gated skip connections](https://www.paperswithcode.com/paper/toward-fast-and-accurate-human-pose)

**time:**  arXiv:2002.11098v1 [cs.CV] 25 Feb 2020

​		本文研究的是高精度、高效率的人体姿态估计。基于近期FCN在解决这一难题方面取得了良好的效果。而在FCNs内的残差连接已被证明是实现高精度的精髓本文重新分析了这种设计，以提高精度和效率。本文提出了可控制的残差连接。提出了结合沙漏和unet的网络。整体网络使用了4个unet结构。截至目前取得了姿态检测的no1。

![image-20200724200443223](.\姿态估计img\image-20200724200443223.png)

同时，对于残差连接，作者添加了一个标量，就是连接控制的权值，通过方向传播学习到。这样就可以算出每一个skp是否要连接。

​	![image-20200724200532845](.\姿态估计img\image-20200724200532845.png)

#### 2.2 CascadePose

**title：**Cascade Feature Aggregation for Human Pose Estimation

**time:**  2019

![image-20200724201340867](.\姿态估计img\image-20200724201340867.png)

#### 2.3 DeepCut

 Joint Subset Partition and Labeling for Multi Person Pose Estimation

#### 2.4 CU-Net

CU-Net: Coupled U-Nets

![image-20200724203110603](.\姿态估计img\image-202007242031102603.png)

#### 2.5 PRMS

Learning Feature Pyramids for Human Pose Estimation

![image-20200724203606238](.\姿态估计img\image-20200724203606238.png)

![image-20200724203630466](.\姿态估计img\image-20200724203630466.png)



#### 2.6 UniPose

![image-20200724204319433](.\姿态估计img\image-20200724204319433.png)

![image-20200724204346532](.\姿态估计img\image-20200724204346532.png)

![image-20200724204359517](.\姿态估计img\image-20200724204359517.png)

![image-20200724204454939](.\姿态估计img\image-20200724204454939.png)



#### 2.7 CPM

![preview](.\姿态估计img\v2-917d5fe6ced68e9300b2735b546ab84e_r.jpg)

#### 2.8 Hourglass

![preview](.\姿态估计img\v2-64a69587aa9c0ad53eba01c9647b448a_r.jpg)

## 四、多人姿态估计

### 1. 简介

​		多人姿态检测一般都分为两大类，一类是自定向下的多人姿态估计算法，这类算法是先区分人，然后再对人进行关键点定位，当前一般都是两阶段的，先用目标检测算法检出人，然后将人的图片重新送入姿态估计算法模型。从本质上来说，这类算法和单人姿态估计没有什么区别。另一种是自底向上的多人姿态估计算法，这类算法是检测出关键点，然后对每个关键点找到其对应的人。

DeepCut

### 2. 论文

#### 2.1 [ CPN](https://www.paperswithcode.com/paper/cascaded-pyramid-network-for-multi-person)

**title：**Cascaded Pyramid Network for Multi-Person Pose Estimation

**time:**  arXiv:1711.07319v2 [cs.CV] 8 Apr 2018

**method:** 自顶向下

​		自顶向下的目标检测方法，先用FPN来检测人体，然后用CPN来拟合关键点。提出了级联的检测器，其中globalnet用来预测全局信息，refinenet用做精细定位。GlobalNet的作用就是简单的前向CNN组成的回归模型，目的是回归得到一些易于识别的人体关键点位置，这里作者是使用的ResNet的网络架构回归heatmap。RefineNet的目的则是利用GlobalNet产生的金字塔模型的特征图，融合多个感受野信息，最后concatenate所有的同一尺寸特征图进一步回归一些有歧义的关键点位置。注意，这里作者使用的是concatenate的策略，而并不是像Stacked Hourglass Networks那样的简单的upsampling。


![image-20200724155307327](.\姿态估计img\image-20200724155307327.png)

**result:**

![image-20200724155556272](.\姿态估计img\image-20200724155556272.png)

#### 2.2 [PersonLab](https://www.paperswithcode.com/paper/personlab-person-pose-estimation-and-instance)

**title：**PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model

**time:**  arXiv:1803.08225v1 [cs.CV] 22 Mar 2018

**method:** 自底向上

​		该文提出了一种自底向上的多人姿态估计算法和实例分割算法。对于人体姿态估计，该文输出三个模块，第一个模块是图像的heatmap，表示关键点，第二个表示短距离的偏移，意思就是在某个关键点周围的圆形内，预测距离中心的偏移，通过该方法，能够准确定位关键点。对于多人姿态估计，还有一个额外的分支，称之为中距离的偏移，用来预测每个关键点对的偏移。

![image-20200724161612694](.\姿态估计img\image-20200724161612694.png)

​		通过短连接，能够定位关键点，中距离能够获取下一个关节点的大体范围，配合短距离，定位关键点。

![img](.\姿态估计img\watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE3NzIxMjM5,size_16,color_FFFFFF,t_70)



**result:**

![image-20200724162559566](.\姿态估计img\image-20200724162559566.png)

#### 2.3 [DirectPose](https://www.paperswithcode.com/paper/directpose-direct-end-to-end-multi-person)

**title：**PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model

**time:**  arXiv:1803.08225v1 [cs.CV] 22 Mar 2018

**method:** 结合

![image-20200724171309217](.\姿态估计img\image-20200724171309217.png)

​		该篇文章也是尝试用anchor free的方法来统一搞定检测和关键点检测，但是单纯的增加2k个通道的方案是不可行的，因此，文章设计了KPAlign模块，虽然最终的输出都是2k，但是该文章提出了Aligner和Predictor模块。该文指出使用单个特征向量来编码实例所需的所有信息。这是困难的，因为很多关键点都远离特征向量接受场的中心，而之前论文显示特征响应强度作为输入信号衰减较快。

​		

![image-20200724171327945](.\姿态估计img\image-20200724171327945.png)

​		这里的KPAlign 讲的不是很明白，但是以本文看来，可以用non-local来替代。



#### 2.3 OpenPose

**title：**Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields ∗

**time:**  CVPR2017

**method:** 自底向上

​		作为2017那个时代最经验的文章之一，值得关注，思路其实很简单，输出下图中的b和c 部分，b是热力图，c是关联图，然后匹配关联关系，关键点匹配，最终能够实现多人姿态估计。

![image-20200724182344854](.\姿态估计img\image-20200724182344854.png)



​		模型来看并不复杂，但是使用了级联检测。L2损失函数

![image-20200724182554428](.\姿态估计img\image-20200724182554428.png)

​		已知一组检测到的身体部位，如何将它们组合成未知人数的全身姿势? 因此，需要对每一对身体部位检测的关联进行置信度测量，判定他们属于同一个人。一种可能的测量关联的方法是在一个分支上的每对部件之间检测一个额外的中点，并检查其在候选部件检测之间的发生率，然而当人们挤在一起的时候，这种方式很容易产生错。为了解决这些限制，该文提出了一种新的特征表示，称为部分亲和场，它在整个肢体的支持区域中保留位置和方向信息。

![img](.\姿态估计img\70)

​		如上图所示，从手臂到手的一块区域内的点均保持方向向量。其中:

![image-20200724183445854](.\姿态估计img\image-20200724183445854.png)

![这里写图片描述](.\姿态估计img\331)

​		评估两个点的相连性,计算从dj1dj1与dj2dj2连线上的线性积分。其中，p(u)是从dj1dj1到dj2dj2连线上的任意一点。：

![img](.\姿态估计img\701)

​		仅仅这些是不够的，我们以知所有关键点，知道其连接情况，所有关键点的匹配问题其实是一个nphard问题，基于此，文章利用先验信息从肩膀处开始拼接。具体方法类似最小生成树的过程。

![img](.\姿态估计img\702)

#### 2.4 [ G-RMI](https://www.paperswithcode.com/paper/towards-accurate-multi-person-pose-estimation)

**title：**Towards Accurate Multi-person Pose Estimation in the Wild

**time:**  CVPR2017

**method:** 自顶向下

![image-20200724184921626](.\姿态估计img\image-20200724184921626.png)

​		思路比较简单，先用faster rcnn检测扣除人，然后用普通的resnet预测热力图和偏移值：
$$
F_{k}(x_{i})=l_k-x_i
$$
​		生成热图和偏移量后，我们将它们聚合，生成高度本地化的激活图，如下：G是双线性插值核，

![image-20200724190742123](C:\Users\duanlunbiao\AppData\Roaming\Typora\typora-user-images\image-20200724190742123.png)

![image-20200724184956171](.\姿态估计img\image-20200724184956171.png)

​		最终给姿态打分，为每个关键点的分数平均，每个关键点为每个最终图的最大值。

#### 2.5 Associative Embedding

ref：https://zhuanlan.zhihu.com/p/45187349



![preview](.\姿态估计img\v2-54ec94c008ff7248fe1387eb5d8418ba_r.jpg)

​    	多人姿势检测与单人姿势检测的区别在于多人的heatmap应具有多个峰值（例如，属于不同人的多个左手腕），而不是单人的单个峰值。为了实现多人姿势检测，网络需要对每个关节点的每个像素位置产生一个标签，也就是说，每个关节点的heatmap对应一个标签heatmap，因此，如果一张图片中待检测的关节点有 ![[公式]](https://www.zhihu.com/equation?tex=m) 个，则网络理想状态下会输出 ![[公式]](https://www.zhihu.com/equation?tex=2m) 个通道， ![[公式]](https://www.zhihu.com/equation?tex=m) 个通道用于定位， ![[公式]](https://www.zhihu.com/equation?tex=m) 个通道用于分组。

​		为了将检测结果对应到个人，作者使用非极大值抑制（non-maximun suppression）来取得每个关节heatmap峰值，然后检索其对应位置的标签，再比较所有身体位置的标签，找到足够接近的标签分为一组，这样就将关节点匹配单个人身上，整个过程如下图所示：

![preview](.\姿态估计img\v2-e46c7910c7b8f0b3ed42177c723ccc87_r.jpg)

**Detection loss**

detection loss使用均方误差，即计算预测的detection heatmap与在关键点加入2D高斯激活的ground truth的heatmap之间的均方误差。

**Grouping loss**

grouping loss衡量的是预测的标签和ground truth分组的标签匹配得有多好，具体说，就是检索图片中所有人的所有身体节点的在相应的ground truth位置的标签，然后比较每个人和人之间的标签，同一个人的标签应该相同，反之，不同人的标签应该不同。

为了减少运算量，我们应该避免直接计算每一对关节点之间的损失，相应的，我们对每个人都产生一个reference embedding，reference embedding的生成方法就是对人的关节点的embedding值取平均。有了reference embedding后，对于单个人来说，我们计算每个关节点预测的embedding和reference embedding的平方距离；对于两个不同的人来说，我们比较他们之间的reference embedding，随着它们之间距离的增加，惩罚将以指数方式降为0。



<img src=".\姿态估计img\v2-71685932ee2ef3cd8da83a11f8de390c_r.jpg" alt="preview" style="zoom:67%;" />

## 五、模型无关姿态估计

### 1. 简介

​		为什么说是模型无关的呢，在很多算法中，不仅仅依赖单纯的检测分割或者其他的直接解决问题的方式，比如数据扩增算法，损失函数设计算法，以及本文接下来会讲的拟合误差的方法，这些算法在很大范围内能够大幅度提升预测性能。

### 2. 论文介绍

#### 2.1 PoseFix

**title：** PoseFix: Model-agnostic General Human Pose Refinement Network

**time:**  CVPR 2019

​		PoseFix截至目前是COCO关键点检测中的第一名，该文将误差统计作为先验信息来生成合成姿态，并使用合成姿态来训练模型。位姿优化主要通过多级架构来实现。换句话说，第一阶段生成的初始位姿和图像特征经过后续阶段后，输出一个精确的位姿。这些多阶段架构通常以端到端方式进行培训。然而，传统的基于结构的多阶段细化方法高度依赖于位姿估计模型，需要精心设计才能成功地细化。相比之下，在本文中提出了一种不依赖模型的姿态细化方法。简单来说，不需要关注位姿模型是什么，只需要知道他的输出，然后拟合误差。类似下图，输入图片和其他位姿预测模型结果，结束纠正后的结果。

![img](.\姿态估计img\posefixresult.jpg)



​		该文将姿态错误估计 分为以下几类：抖动(jitter) , 反转(inversion) , 交换(swap) , 缺失(miss)，并记录这些错误出现的频率：获得了错误统计信息(error statistics)，然后将这些信息作为先验信息来生成姿态(synthetic poses)，然后用这些姿态来训练提出的姿态优化模型(the proposed pose refinement model (PoseFix))。

​		如下图所示，训练输入是增加先验错误的热力图，和原图结合，经过普通的前向网络最终得到热力图，然后经过argmax获取关节点。除此之外，作者还测试了粗到精估计，即输入的是一个热力图，输出的是一个关键点，这是一个由粗到细的过程。

![在这里插入图片描述](.\姿态估计img\watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dhbmd5YzEyMDg=,size_16,color_FFFFFF,t_70)
**result:**

![image-20200724140604697](.\姿态估计img\image-20200724140604697.png)



#### 2.2 DarkPose

**title：**Distribution-Aware Coordinate Representation for Human Pose Estimations

**time:**  arXiv:1910.06278v1 [cs.CV] 14 Oct 2019

​		该文发现将预测的热图解码为原始图像空间中的最终关节坐标的过程，对人体姿态估计的性能有着惊人的重要意义。简单来说是传统的热力图直接获取关节点的方式会出现偏差，该文针对该问题，提出了另一种模型无关的提升精度的方法。

​		具体来说热力图预测是一个将预测的每个关节的热图转换为原始图像空间坐标的过程。假设热图与原始图像具有相同的空间大小，我们只需要找到最大激活的位置作为关节坐标预测，简单直观。然而，这通常不是一般情况。相反，我们需要通过采样特定的无约束因子来对热图进行采样，使其达到原始图像的分辨率。这涉及到一个亚像素的本地化问题。

​		一般的做法是如下公式，其中m是最大相应点，s是次大相应点。这种方式非常有效。
$$
\boldsymbol{p}=\boldsymbol{m}+0.25 \frac{\boldsymbol{s}-\boldsymbol{m}}{\|\boldsymbol{s}-\boldsymbol{m}\|_{2}}
$$
​		然而这种方法是没有理论依据的，作者提出了另一种带着强烈理论的方法。其中D表示在m位置的一阶、二阶导数，m为最高点位置，具体推导见原文。

![Darkpose2.jpg](.\姿态估计img\2649431699.jpg)



![image-20200724151646824](.\姿态估计img\image-20200724151646824.png)

**result:**

![image-20200724152249655](.\姿态估计img\image-20200724152249655.png)

## 六、引用



#### website：

1. https://www.paperswithcode.com/area/computer-vision/pose-estimation

2. https://zhuanlan.zhihu.com/p/85506259

3. https://www.jianshu.com/p/f6d92887d8c6
4. https://zhuanlan.zhihu.com/p/104917833

待整理： 数据集，任务介绍

